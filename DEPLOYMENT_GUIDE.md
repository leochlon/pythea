# Glass Production Deployment Guide

Complete guide for deploying Glass in production environments.

---

## üéØ Deployment Scenarios

### Scenario 1: Cloud API (OpenAI/Anthropic)
**Best for:** Fast response times, high reliability
**Cost:** ~$0.001 per query
**Setup time:** < 5 minutes

### Scenario 2: Local Models (Ollama)
**Best for:** Privacy, zero cost, offline
**Cost:** $0 (hardware only)
**Setup time:** ~30 minutes

### Scenario 3: Hybrid (Glass + Fallback)
**Best for:** Balance of speed and accuracy
**Cost:** Variable (mostly Glass = cheap)
**Setup time:** ~15 minutes

---

## üì¶ Installation

### Option 1: From Source (Development)

```bash
# Clone repository
git clone https://github.com/your-org/hallbayes.git
cd hallbayes

# Install dependencies
pip install -e .
pip install requests  # For Ollama support

# Verify installation
python -c "from glass import GlassPlanner; print('‚úì Glass installed')"
```

### Option 2: Docker (Production)

```bash
# Build Docker image
docker build -t glass-api -f docker/Dockerfile .

# Run container
docker run -p 8000:8000 \
  -e OPENAI_API_KEY=sk-... \
  glass-api

# Or with Ollama
docker run -p 8000:8000 \
  --add-host=host.docker.internal:host-gateway \
  glass-api
```

### Option 3: pip install (Future)

```bash
# When published to PyPI
pip install hallbayes[glass]
```

---

## ‚öôÔ∏è Configuration

### Environment Variables

```bash
# Cloud providers
export OPENAI_API_KEY=sk-...
export ANTHROPIC_API_KEY=sk-ant-...

# Ollama (local)
export OLLAMA_HOST=http://localhost:11434

# Glass settings
export GLASS_SYMMETRY_THRESHOLD=0.6
export GLASS_TEMPERATURE=0.3
export GLASS_MAX_TOKENS=256
export GLASS_CACHE_SIZE=1000
```

### Configuration File

Create `glass_config.yaml`:

```yaml
# Backend configuration
backend:
  type: openai  # openai, anthropic, ollama
  model: gpt-4o-mini
  temperature: 0.3
  timeout: 60.0

# Glass settings
glass:
  symmetry_threshold: 0.6
  max_tokens: 256
  cache_enabled: true
  cache_size: 1000
  cache_ttl_hours: 24

# Hybrid mode settings
hybrid:
  enabled: true
  glass_confidence_threshold: 0.7
  use_fallback: true

# Monitoring
monitoring:
  enabled: true
  log_level: INFO
  metrics_enabled: true
  export_prometheus: true
```

---

## üöÄ Deployment Options

### 1. REST API (FastAPI)

Create `api/main.py`:

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import time

from hallbayes import OpenAIBackend
from glass import GlassPlanner, GlassItem

app = FastAPI(title="Glass API", version="1.0.0")

# Initialize backend and planner
backend = OpenAIBackend(model="gpt-4o-mini")
planner = GlassPlanner(backend, temperature=0.3)

class EvaluateRequest(BaseModel):
    prompts: List[str]
    h_star: float = 0.05
    symmetry_threshold: float = 0.6

class EvaluateResponse(BaseModel):
    results: List[dict]
    total_time: float
    average_time: float

@app.post("/evaluate", response_model=EvaluateResponse)
async def evaluate(request: EvaluateRequest):
    """Evaluate prompts for hallucination risk"""
    try:
        start = time.time()

        items = [
            GlassItem(
                prompt=p,
                symmetry_threshold=request.symmetry_threshold
            )
            for p in request.prompts
        ]

        metrics = planner.run(items, h_star=request.h_star)
        elapsed = time.time() - start

        results = [
            {
                "prompt": request.prompts[i],
                "decision": "answer" if m.decision_answer else "refuse",
                "symmetry_score": m.symmetry_score,
                "isr": m.isr,
                "roh_bound": m.roh_bound,
            }
            for i, m in enumerate(metrics)
        ]

        return EvaluateResponse(
            results=results,
            total_time=elapsed,
            average_time=elapsed / len(request.prompts)
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health():
    """Health check endpoint"""
    return {"status": "healthy", "service": "glass-api"}

@app.get("/info")
async def info():
    """API information"""
    return {
        "version": "1.0.0",
        "backend": backend.model,
        "glass_enabled": True,
    }
```

Run the API:

```bash
# Install FastAPI
pip install fastapi uvicorn

# Run server
uvicorn api.main:app --host 0.0.0.0 --port 8000

# Test
curl -X POST http://localhost:8000/evaluate \
  -H "Content-Type: application/json" \
  -d '{"prompts": ["Who won the 2019 Nobel Prize?"]}'
```

### 2. AWS Lambda (Serverless)

Create `lambda/handler.py`:

```python
import json
import os
from hallbayes import OpenAIBackend
from glass import GlassPlanner, GlassItem

# Initialize outside handler for cold start optimization
backend = OpenAIBackend(
    model=os.environ.get("MODEL", "gpt-4o-mini")
)
planner = GlassPlanner(backend, temperature=0.3)

def lambda_handler(event, context):
    """AWS Lambda handler for Glass"""
    try:
        body = json.loads(event['body'])
        prompts = body.get('prompts', [])

        items = [GlassItem(prompt=p) for p in prompts]
        metrics = planner.run(items, h_star=0.05)

        results = [
            {
                "decision": "answer" if m.decision_answer else "refuse",
                "symmetry": m.symmetry_score,
                "isr": m.isr,
            }
            for m in metrics
        ]

        return {
            'statusCode': 200,
            'body': json.dumps(results)
        }

    except Exception as e:
        return {
            'statusCode': 500,
            'body': json.dumps({'error': str(e)})
        }
```

Deploy:

```bash
# Package dependencies
pip install -t package/ hallbayes
cd package && zip -r ../deployment.zip . && cd ..
zip -g deployment.zip lambda/handler.py

# Deploy with AWS CLI
aws lambda create-function \
  --function-name glass-eval \
  --runtime python3.9 \
  --handler lambda.handler.lambda_handler \
  --zip-file fileb://deployment.zip \
  --environment Variables="{OPENAI_API_KEY=sk-...}"
```

### 3. Docker Container

Create `Dockerfile`:

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY hallbayes/ /app/hallbayes/
COPY glass/ /app/glass/
COPY api/ /app/api/

# Expose port
EXPOSE 8000

# Run API
CMD ["uvicorn", "api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

Build and run:

```bash
docker build -t glass-api .
docker run -p 8000:8000 -e OPENAI_API_KEY=sk-... glass-api
```

### 4. Kubernetes Deployment

Create `k8s/deployment.yaml`:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: glass-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: glass-api
  template:
    metadata:
      labels:
        app: glass-api
    spec:
      containers:
      - name: glass-api
        image: glass-api:latest
        ports:
        - containerPort: 8000
        env:
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: api-keys
              key: openai-key
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
---
apiVersion: v1
kind: Service
metadata:
  name: glass-api-service
spec:
  selector:
    app: glass-api
  ports:
  - port: 80
    targetPort: 8000
  type: LoadBalancer
```

Deploy:

```bash
kubectl apply -f k8s/deployment.yaml
kubectl apply -f k8s/service.yaml
```

---

## üìä Monitoring & Logging

### Logging Setup

```python
import logging
from glass import GlassPlanner

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('glass.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger('glass')

# Custom wrapper with logging
class MonitoredPlanner:
    def __init__(self, backend):
        self.planner = GlassPlanner(backend)
        self.logger = logger

    def run(self, items, **kwargs):
        self.logger.info(f"Starting evaluation of {len(items)} items")

        import time
        start = time.time()

        try:
            metrics = self.planner.run(items, **kwargs)
            elapsed = time.time() - start

            answered = sum(1 for m in metrics if m.decision_answer)
            self.logger.info(
                f"Completed: {len(items)} items, "
                f"{answered} answered, "
                f"{elapsed:.2f}s"
            )

            return metrics

        except Exception as e:
            self.logger.error(f"Evaluation failed: {e}")
            raise
```

### Prometheus Metrics

```python
from prometheus_client import Counter, Histogram, start_http_server

# Define metrics
evaluations_total = Counter(
    'glass_evaluations_total',
    'Total number of evaluations'
)

evaluation_duration = Histogram(
    'glass_evaluation_duration_seconds',
    'Time spent evaluating prompts'
)

decisions_total = Counter(
    'glass_decisions_total',
    'Total decisions made',
    ['decision']  # answer or refuse
)

# Instrument code
class MetricsPlanner:
    def __init__(self, backend):
        self.planner = GlassPlanner(backend)

    @evaluation_duration.time()
    def run(self, items, **kwargs):
        evaluations_total.inc()

        metrics = self.planner.run(items, **kwargs)

        for m in metrics:
            decision = 'answer' if m.decision_answer else 'refuse'
            decisions_total.labels(decision=decision).inc()

        return metrics

# Start metrics server
start_http_server(9090)
```

---

## üîí Security Best Practices

### 1. API Key Management

```python
import os
from dotenv import load_dotenv

# Load from .env file (never commit!)
load_dotenv()

api_key = os.environ.get('OPENAI_API_KEY')
if not api_key:
    raise ValueError("OPENAI_API_KEY not set")
```

### 2. Rate Limiting

```python
from fastapi import FastAPI, HTTPException
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)
app = FastAPI()
app.state.limiter = limiter
app.add_exception_handler(429, _rate_limit_exceeded_handler)

@app.post("/evaluate")
@limiter.limit("10/minute")  # 10 requests per minute
async def evaluate(request: Request, ...):
    # ... evaluation logic
    pass
```

### 3. Input Validation

```python
from pydantic import BaseModel, validator

class EvaluateRequest(BaseModel):
    prompts: List[str]

    @validator('prompts')
    def validate_prompts(cls, v):
        if not v:
            raise ValueError("prompts cannot be empty")
        if len(v) > 100:
            raise ValueError("maximum 100 prompts per request")
        for prompt in v:
            if len(prompt) > 10000:
                raise ValueError("prompt too long (max 10000 chars)")
        return v
```

---

## ‚ö° Performance Optimization

### 1. Caching

```python
from glass.cache import CachedGrammaticalMapper
from glass import GlassPlanner

# Enable caching
mapper = CachedGrammaticalMapper(
    cache_size=10000,
    cache_ttl_hours=24
)

# Inject into planner
planner = GlassPlanner(backend)
planner.mapper = mapper  # Use cached mapper

# Monitor cache performance
stats = mapper.get_cache_stats()
print(f"Cache hit rate: {stats['hit_rate']*100:.1f}%")
```

### 2. Batch Processing

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

async def evaluate_batch_parallel(planner, items, batch_size=10):
    """Evaluate items in parallel batches"""
    results = []

    for i in range(0, len(items), batch_size):
        batch = items[i:i+batch_size]

        # Run batch in thread pool
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor() as executor:
            batch_results = await loop.run_in_executor(
                executor,
                planner.run,
                batch
            )

        results.extend(batch_results)

    return results
```

### 3. Connection Pooling

```python
from hallbayes import OpenAIBackend
import openai

# Configure connection pool
backend = OpenAIBackend(
    model="gpt-4o-mini",
    request_timeout=30.0
)

# Reuse backend across requests (singleton pattern)
_planner_instance = None

def get_planner():
    global _planner_instance
    if _planner_instance is None:
        _planner_instance = GlassPlanner(backend)
    return _planner_instance
```

---

## üß™ Testing in Production

### Health Checks

```python
@app.get("/health")
async def health_check():
    """Comprehensive health check"""
    try:
        # Test backend connectivity
        test_item = GlassItem(prompt="test")
        _ = planner.evaluate_item(0, test_item)

        return {
            "status": "healthy",
            "backend": backend.model,
            "timestamp": time.time()
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": time.time()
        }
```

### Load Testing

```bash
# Install locust
pip install locust

# Run load test
locust -f tests/load_test.py --host=http://localhost:8000
```

---

## üìà Scaling Strategies

### Horizontal Scaling

```yaml
# Kubernetes HPA
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: glass-api-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: glass-api
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

### Vertical Scaling

For Ollama (local models), use larger instances:
- **Small:** 4 CPU, 8GB RAM ‚Üí 1-2 concurrent requests
- **Medium:** 8 CPU, 16GB RAM ‚Üí 3-5 concurrent requests
- **Large:** 16 CPU, 32GB RAM ‚Üí 6-10 concurrent requests

---

## üí∞ Cost Optimization

### Strategy 1: Hybrid Mode

```python
# 75% of queries use Glass (cheap)
# 25% fallback to Original (expensive)
# Average cost: ~$0.008 vs $0.03 (73% savings)

from glass.example_hybrid import HybridPlanner

planner = HybridPlanner(
    backend,
    glass_confidence_threshold=0.7,
    use_fallback=True
)
```

### Strategy 2: Ollama for Development

```python
# Development: Use Ollama (free)
if os.environ.get('ENV') == 'development':
    backend = OllamaBackend(model="llama3.1:8b")
# Production: Use OpenAI (fast, reliable)
else:
    backend = OpenAIBackend(model="gpt-4o-mini")
```

### Strategy 3: Caching

```python
# Cache hit rate of 40% = 40% cost savings
mapper = CachedGrammaticalMapper(cache_size=10000)
```

---

## üîç Troubleshooting

### Common Issues

**Issue:** Slow response times with Ollama
**Solution:** Increase `request_timeout` and use GPU if available

**Issue:** High memory usage
**Solution:** Reduce `cache_size` or disable caching

**Issue:** Rate limiting errors
**Solution:** Implement exponential backoff and request queuing

**Issue:** Inconsistent decisions
**Solution:** Tune `symmetry_threshold` based on validation data

---

## üìö Additional Resources

- **API Documentation:** See `api/` directory
- **Docker Examples:** See `docker/` directory
- **Kubernetes Configs:** See `k8s/` directory
- **Monitoring Dashboard:** See `monitoring/` directory

---

## üöÄ Quick Start Commands

```bash
# Local development
python -m glass.example_quick_start

# API server
uvicorn api.main:app --reload

# Docker deployment
docker-compose up

# Kubernetes deployment
kubectl apply -f k8s/

# Run tests
pytest tests/

# Load testing
locust -f tests/load_test.py
```

---

**Glass is production-ready! Deploy with confidence.** üöÄ
