Metadata-Version: 2.4
Name: pythea
Version: 0.3.0
Summary: LLM reliability research: procedural hallucination detection + Thea API client
Author: Hassana Labs
License: MIT
Requires-Python: >=3.9
Description-Content-Type: text/markdown
License-File: LICENSE.md
Requires-Dist: httpx>=0.24.0
Requires-Dist: openai>=1.0.0
Requires-Dist: numpy>=1.20.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.21.0; extra == "dev"
Provides-Extra: offline
Requires-Dist: tiktoken>=0.6.0; extra == "offline"
Provides-Extra: vllm
Requires-Dist: vllm>=0.4.0; extra == "vllm"
Dynamic: license-file

# pythea

**LLM reliability research from [Hassana Labs](https://hassana.io).**

Three tools, one goal: catch models that know but don't use.

---

## ğŸ“ Strawberry: Procedural Hallucination Toolkit

Ask Claude to count the r's in "strawberry." It writes "s-t-r-a-w-b-e-r-r-y," identifies each r, gets to 3. Then outputs "2."

The model didn't lack information. The answer was right thereâ€”in text it generated moments earlier. The computation worked. The routing failed.

This toolkit detects those failures mathematically.

```bash
pip install pythea
python -m strawberry.factual_recall \
  --question "Which US senators from Minnesota graduated from Princeton" \
  --out report.json
```

**What it catches:**
- RAG that retrieves but doesn't read
- Chain-of-thought that cites steps it ignored
- Self-verification that validates without checking
- Citation confabulation (decorative sources)

**How:** Scrub the cited evidence, measure confidence change. No change? The model was confabulating.

**Integrations:** Strawberry ships with an MCP server (`strawberry.mcp_server`) that can be used from
Claude Code *or* OpenAI Codex (Codex CLI / IDE extension) to run `detect_hallucination` and
`audit_trace_budget` as tools.

**Agent workflow:** This repo also includes a Codex skill, `rca-fix-agent`, that turns Codex into an
evidence-first debugging agent: reproduce â†’ gather evidence â†’ test hypotheses â†’ verify the
ROOT_CAUSE claim with Strawberry â†’ implement a fix â†’ run a test plan â†’ check regressions â†’ iterate
until it actually works.

[â†’ Full docs](./strawberry/README.md)

---

## ğŸ”Œ Thea API Client

Lightweight client for the Thea Mini Reasoning API.

```python
from pythea import TheaClient

with TheaClient(base_url="https://...") as client:
    resp = client.unified_answer(
        question="What is 2+2?",
        backend="aoai-pool",
        m=6,
    )
    print(resp.get("answer"))
```

[â†’ Full docs](./docs/CLIENT.md)

---

## ğŸ“Š Offline QMV Probing

Model-agnostic permutation-mixture evaluation via Bernoulli first-token logprob probes.

```python
from pythea.offline import qmv

res = qmv.evaluate_permutation_family(
    probe=probe,
    parts=parts,
    cfg=qmv.PermutationEvalConfig(m=6, num_bands=2, seed=0),
)
print(res.q_bar, res.q_lo, res.js_bound)
```

[â†’ Full docs](./docs/QMV.md)

---

## Install

```bash
git clone https://github.com/leochlon/pythea.git
cd pythea
pip install -e .
```

**Extras:**
```bash
pip install -e ".[dev]"      # tests
pip install -e ".[offline]"  # tiktoken for logit bias
pip install -e ".[vllm]"     # local inference
```

---

## Repo layout

```
pythea/
â”œâ”€â”€ strawberry/          # Procedural hallucination toolkit
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ src/
â”œâ”€â”€ src/pythea/          # Thea client + QMV probing
â”œâ”€â”€ docs/                # Detailed documentation
â”œâ”€â”€ examples/
â”œâ”€â”€ tests/
â””â”€â”€ benchmarks/
```

---

## Citation

```bibtex
@article{hassanalabs2026procedural,
  title={An Information-Theoretic and Causal Theory of Procedural Hallucinations},
  author={{Hassana Labs}},
  journal={arXiv preprint},
  year={2026}
}
```

---

## License

MIT â€” see [LICENSE.md](./LICENSE.md)
