#!/usr/bin/env python3
"""
Glass + Ollama Benchmark
========================

Complete benchmark of Glass with local Ollama llama3.1:8b model.

Tests:
- Multiple prompt types
- Performance metrics
- Symmetry analysis
- Decision distribution
"""

import sys
import time
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent))

from hallbayes.htk_backends import OllamaBackend
from glass import GlassPlanner, GlassItem
from glass.visualizer import print_batch_results


# Test prompts covering different categories
TEST_PROMPTS = [
    # === Factual - Easy ===
    "What is the capital of France?",
    "What is 2+2?",
    "Who wrote Romeo and Juliet?",

    # === Factual - Medium ===
    "Who won the 2019 Nobel Prize in Physics?",
    "When did World War II end?",
    "What is the speed of light?",

    # === Reasoning ===
    "Explain the concept of photosynthesis.",
    "What are the main differences between DNA and RNA?",

    # === Ambiguous ===
    "What is the fastest animal?",
    "How many planets are in the solar system?",
]


def print_header(title):
    """Print section header"""
    print("\n" + "=" * 70)
    print(title)
    print("=" * 70)


def main():
    print_header("GLASS + OLLAMA BENCHMARK")
    print(f"\nModel: llama3.1:8b (local)")
    print(f"Test prompts: {len(TEST_PROMPTS)}")
    print(f"Privacy: 100% local - no API calls")
    print(f"Cost: $0")

    # Initialize
    print("\n1. Initializing Ollama backend...")
    try:
        backend = OllamaBackend(
            model="llama3.1:8b",
            host="http://localhost:11434",
            request_timeout=180.0
        )
        print(f"   ‚úì Backend ready: {backend.model}")
    except Exception as e:
        print(f"   ‚úó Error: {e}")
        print("\n   Make sure Ollama is running:")
        print("   $ ollama serve")
        return 1

    # Create planner
    print("\n2. Creating Glass planner...")
    planner = GlassPlanner(backend, temperature=0.3, verbose=False)
    print("   ‚úì Planner ready")

    # Create items
    items = [GlassItem(prompt=p) for p in TEST_PROMPTS]

    # Run benchmark
    print(f"\n3. Running benchmark ({len(items)} items)...")
    print("   This will take ~5-8 minutes (local inference is slower)")
    print("   Progress:")

    start_time = time.time()
    metrics = []

    for i, item in enumerate(items, 1):
        item_start = time.time()

        try:
            result = planner.evaluate_item(i-1, item, h_star=0.05)
            metrics.append(result)
            item_time = time.time() - item_start

            decision = "‚úì ANSWER" if result.decision_answer else "‚úó REFUSE"
            print(f"   [{i}/{len(items)}] {decision} - {item_time:.1f}s - Sym: {result.symmetry_score:.3f}")

        except Exception as e:
            print(f"   [{i}/{len(items)}] ‚úó ERROR: {e}")
            continue

    total_time = time.time() - start_time

    # Calculate statistics
    print_header("RESULTS")

    answered = sum(1 for m in metrics if m.decision_answer)
    refused = len(metrics) - answered

    symmetry_scores = [m.symmetry_score for m in metrics if hasattr(m, 'symmetry_score')]
    avg_symmetry = sum(symmetry_scores) / len(symmetry_scores) if symmetry_scores else 0

    isr_scores = [m.isr for m in metrics]
    avg_isr = sum(isr_scores) / len(isr_scores) if isr_scores else 0

    # Print summary
    print(f"\nüìä Overall Statistics:")
    print(f"   Total items:        {len(metrics)}")
    print(f"   Answered:           {answered} ({answered/len(metrics)*100:.1f}%)")
    print(f"   Refused:            {refused} ({refused/len(metrics)*100:.1f}%)")
    print(f"   Average symmetry:   {avg_symmetry:.3f}")
    print(f"   Average ISR:        {avg_isr:.2f}")

    print(f"\n‚è±Ô∏è  Performance:")
    print(f"   Total time:         {total_time:.2f}s ({total_time/60:.1f} min)")
    print(f"   Time per item:      {total_time/len(metrics):.2f}s")
    print(f"   Throughput:         {len(metrics)/total_time:.2f} items/s")

    print(f"\nüí∞ Cost Analysis:")
    print(f"   API calls:          {len(metrics)} (Glass: 1 per item)")
    print(f"   API cost:           $0 (completely local)")
    print(f"   Privacy:            ‚úÖ 100% local - data never leaves machine")

    # Detailed results
    print_header("DETAILED RESULTS")

    for i, (prompt, m) in enumerate(zip(TEST_PROMPTS, metrics), 1):
        decision = "‚úì ANSWER" if m.decision_answer else "‚úó REFUSE"
        sym = f"{m.symmetry_score:.3f}" if hasattr(m, 'symmetry_score') else "N/A"

        print(f"\n[{i}] {prompt}")
        print(f"    Decision:  {decision}")
        print(f"    Symmetry:  {sym}")
        print(f"    ISR:       {m.isr:.2f}")
        print(f"    RoH bound: {m.roh_bound:.3f}")

    # Symmetry distribution
    print_header("SYMMETRY DISTRIBUTION")

    if symmetry_scores:
        bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]
        for i in range(len(bins)-1):
            count = sum(1 for s in symmetry_scores if bins[i] <= s < bins[i+1])
            bar = "‚ñà" * count
            print(f"   {bins[i]:.1f}-{bins[i+1]:.1f}: {bar} ({count})")

    # Conclusion
    print_header("CONCLUSION")

    print(f"""
‚úÖ Glass successfully processed {len(metrics)} prompts with Ollama llama3.1:8b

Key Findings:
  ‚Ä¢ Performance: {total_time/len(metrics):.1f}s per query (local model)
  ‚Ä¢ Decision rate: {answered/len(metrics)*100:.0f}% answered
  ‚Ä¢ Privacy: 100% local - no data sent to cloud
  ‚Ä¢ Cost: $0 - completely free

Comparison to Cloud (OpenAI):
  ‚Ä¢ Cloud speed: ~1s per query (30√ó faster)
  ‚Ä¢ Cloud cost: ~$0.001 per query
  ‚Ä¢ Local advantage: Complete privacy, zero cost, offline capable

Glass + Ollama is perfect for:
  ‚úì Privacy-sensitive applications
  ‚úì Zero-cost operations
  ‚úì Offline deployments
  ‚úì High-volume processing (despite slower per-query time)
    """)

    print("=" * 70)
    print("\n‚úÖ Benchmark complete!\n")

    return 0


if __name__ == "__main__":
    sys.exit(main())
